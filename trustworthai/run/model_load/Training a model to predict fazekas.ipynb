{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a7216b1-f979-4921-b472-a2594968d7a5",
   "metadata": {},
   "source": [
    "### Prediction of Fazekas/clinical scores from uncertainty\n",
    "\n",
    "we will use the full uncertianty map, on a few selected slices, and attempt to train a model to predict fazekas or clinical scores, and see what they show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5a707b8-9797-4de2-b8ac-7f8d279565da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strawberry\n",
      "banana\n"
     ]
    }
   ],
   "source": [
    "# trainer\n",
    "print(\"strawberry\")\n",
    "from trustworthai.utils.fitting_and_inference.fitters.basic_lightning_fitter import StandardLitModelWrapper\n",
    "from trustworthai.utils.fitting_and_inference.get_trainer import get_trainer\n",
    "\n",
    "# data\n",
    "from trustworthai.utils.data_preprep.dataset_pipelines import load_clinscores_data, load_data, ClinScoreDataRetriever\n",
    "from torch.utils.data import ConcatDataset, Dataset, DataLoader\n",
    "\n",
    "# packages\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchinfo import summary\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from natsort import natsorted\n",
    "import torchmetrics\n",
    "print(\"banana\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e516a065-4175-491a-a14e-d4db8f5bf455",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(12, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4177f5b2-1ce4-48cb-9fa2-92cf3b7fef0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.2909899950027466, -0.29722467064857483, 0.2797778248786926], [-1.3678717613220215, 0.6004063487052917, 0.8180018663406372], [0.681077778339386, 0.8565403819084167, 0.7433484792709351], [0.4244858920574188, -1.2686867713928223, -1.2010217905044556], [0.4175659418106079, -0.7870716452598572, 0.4505966901779175], [-0.20952408015727997, 1.4196722507476807, -1.0284515619277954], [0.054718125611543655, 0.08008485287427902, 1.0245919227600098], [0.18501140177249908, -0.5533344745635986, 0.23479677736759186], [-1.3929345607757568, -0.783789873123169, 0.261030375957489], [0.4799298942089081, 1.4396705627441406, -0.07006285339593887], [-1.6289207935333252, -0.7018457055091858, 1.0820434093475342], [1.6048171520233154, 0.16421790421009064, -0.8603100180625916]]\n"
     ]
    }
   ],
   "source": [
    "print(a.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37d0534-0197-4b10-a085-93752e46e128",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4eb27a16-a336-46fd-a193-9507d9e1fc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load the data\n",
    "clin_retriever = ClinScoreDataRetriever(use_updated_scores=True)\n",
    "split = 0\n",
    "train_ds_clin, val_ds_clin, test_ds_clin = clin_retriever.load_clinscores_data(\n",
    "        combine_all=False,\n",
    "        test_proportion=0.15, \n",
    "        validation_proportion=0.12,\n",
    "        seed=3407,\n",
    "        cross_validate=True,\n",
    "        cv_split=split,\n",
    "        cv_test_fold_smooth=1,\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf48d3aa-8952-4991-b16c-61def74468c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test_ds_clin[0][2].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "340afc16-e200-47c9-ac59-3ddfc239f4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "r['smoking_0'] = (r['smoking'] == 0).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4523af27-ca2e-4f51-9ad4-af5008448f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                 112\n",
       "ID                      CVD152\n",
       "sbjGroup                     1\n",
       "age                       81.0\n",
       "sex                          1\n",
       "diabetes                     0\n",
       "hypertension                 1\n",
       "hyperlipidaemia            1.0\n",
       "smoking                    1.0\n",
       "SBP                      170.0\n",
       "DBP                       84.0\n",
       "totalChl                   4.2\n",
       "MMSE                       NaN\n",
       "ICV                1388.830933\n",
       "WMH                  27.970046\n",
       "PVWMH                      2.0\n",
       "DWMH                       1.0\n",
       "deepAtrophy                6.0\n",
       "supAtrophy                 2.0\n",
       "BGPVS                      3.0\n",
       "CSPVS                      2.0\n",
       "relLes                       1\n",
       "oldLes                     0.0\n",
       "micrBld                    1.0\n",
       "smoking_0                  0.0\n",
       "Name: 112, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54868b0b-2dad-4600-97a4-cf6ada19cf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomClinFieldsDataset(torch.utils.data.Dataset):\n",
    "    # this is just a wrapper on the clinical dataset where I am hard coding a few things, such as one hot encodings, and all the other stuff that\n",
    "    # I might want to compute like tab fazekas and total fazekas etc\n",
    "    def __init__(self, base_dataset):\n",
    "        super().__init__()\n",
    "        self.base_dataset = base_dataset\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        x, y, clin_data = self.base_dataset[idx]\n",
    "        r = clin_data.copy()\n",
    "        \n",
    "        # smoking\n",
    "        r['smoking_0'] = (r['smoking'] == 0).astype(np.float32)\n",
    "        r['smoking_1'] = (r['smoking'] == 1).astype(np.float32)\n",
    "        r['smoking_2'] = (r['smoking'] == 2).astype(np.float32)\n",
    "        \n",
    "        # total fazekas and scale fazekas\n",
    "        dwmh = r['DWMH']\n",
    "        pvwmh = r['PVWMH']\n",
    "        total_fazekas = np.nan\n",
    "        if (not np.isnan(dwmh)) and (not np.isnan(pvwmh)):\n",
    "            total_fazekas = dwmh + pvwmh\n",
    "        r['total_fazekas'] = total_fazekas\n",
    "        r['scale_fazekas'] = ((pvwmh == 3) | (dwmh > 1)).astype(np.float32)\n",
    "        \n",
    "        # scale PVS\n",
    "        bgpvs = r['BGPVS']\n",
    "        scale_pvs = np.nan\n",
    "        if not np.isnan(bgpvs):\n",
    "            scale_pvs = (bgpvs >= 2).astype(np.float32)\n",
    "        r['scale_pvs'] = scale_pvs\n",
    "        \n",
    "        # scale micrBld\n",
    "        micrbld = r['micrBld']\n",
    "        scale_micrbld = np.nan\n",
    "        if not np.isnan(scale_micrbld):\n",
    "            scale_micrbld = (micrbld > 0).astype(np.float32)\n",
    "        r['scale_micrbld'] = scale_micrbld\n",
    "        \n",
    "        # stroke_les and scale stroke\n",
    "        oldLes = r['oldLes']\n",
    "        relLes = r['relLes']\n",
    "        stroke_les = np.nan\n",
    "        scale_stroke = np.nan\n",
    "        if not np.isnan(oldLes):\n",
    "            if type(relLes) == str:\n",
    "                if relLes != ' ':\n",
    "                    relLes = float(relLes)\n",
    "                else:\n",
    "                    relLes = 0.0\n",
    "            if type(oldLes) == str:\n",
    "                if oldLes != ' ':\n",
    "                    oldLes = float(oldLes)\n",
    "                else:\n",
    "                    oldLes = 0.0\n",
    "            try:\n",
    "                if np.isnan(relLes):\n",
    "                    relLes = 0.0\n",
    "            except:\n",
    "                print(\"failed on : \", relLes, type(relLes))\n",
    "            \n",
    "            try:\n",
    "                stroke_les = ((oldLes ==1)| (relLes==1)).astype(np.float32)\n",
    "            except:\n",
    "                print(f\"failed: old:{oldLes}, rel:{relLes}\")\n",
    "            scale_stroke = (oldLes * relLes).astype(np.float32)\n",
    "        r['stroke_les'] = stroke_les\n",
    "        r['scale_stroke'] = scale_stroke\n",
    "            \n",
    "        return x, y, r\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69328df4-26cb-4d38-99cb-db40ecc69e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load the extra information (e.g uncertainty maps, var maps, preds etc)\n",
    "umap_model_name = \"SSN_Ens\"\n",
    "output_maps_dir = f\"/home/s2208943/ipdis/data/preprocessed_data/EdData_output_maps/{umap_model_name}/\"\n",
    "\n",
    "output_maps_files = os.listdir(output_maps_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "179c7df3-433f-43a6-aecf-d8e1298c23d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = train_ds_clin\n",
    "\n",
    "def get_output_maps_for_ds(ds):\n",
    "    output_maps_lists = defaultdict(lambda : [])\n",
    "    for data in tqdm(ds, position=0, leave=True):\n",
    "        ID = data[2]['ID']\n",
    "        output_maps_data = np.load(f\"{output_maps_dir}{ID}_out_maps.npz\")\n",
    "        for output_type in output_maps_data.keys():\n",
    "            output_maps_lists[output_type].append(torch.from_numpy(output_maps_data[output_type]))\n",
    "            \n",
    "    return output_maps_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9e9df8a-19df-4ad6-9785-5b6658b01266",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_maps_test = get_output_maps_for_ds(test_ds_clin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68a53c28-50e4-4718-a752-863fec367519",
   "metadata": {},
   "outputs": [],
   "source": [
    "### dataset to load in the uncertainty maps and prediction data and add it into the existing dataset.\n",
    "\n",
    "class AddChannelsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_dataset, extra_x_channels_lists):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.extra_x_channels_lists = extra_x_channels_lists\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.base_dataset[idx]\n",
    "        x = data[0]\n",
    "        x = torch.cat([x, *[self.extra_x_channels_lists[key][idx].unsqueeze(0) for key in natsorted(self.extra_x_channels_lists.keys())]], dim=0)\n",
    "        \n",
    "        return (x, *data[1:])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b2ef5d6-f775-4f09-a7a1-d2d7a478ff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#added_channels_testds = AddChannelsDataset(test_ds_clin, output_maps_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d45f59f6-7c48-48b5-995e-ea655407fdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_ds_clin[0][0].shape, added_channels_testds[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6918fa0-595e-4c25-a42c-fcd51b38b014",
   "metadata": {},
   "outputs": [],
   "source": [
    "### code to select slices - will need to do that randomly, and implement an augmentation procedure (I should add noise maybe)\n",
    "\n",
    "# a bunch of stuff that I generated with chat-gpt\n",
    "\n",
    "def find_largest_slice(tensor):\n",
    "    \"\"\"\n",
    "    Find the slice of a PyTorch tensor with the largest sum.\n",
    "    \n",
    "    Args:\n",
    "        tensor: A PyTorch tensor of shape (S, H, W).\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (slice_H, slice_W) representing the slice with the largest sum.\n",
    "    \"\"\"\n",
    "    # Compute the sum along the H and W dimensions\n",
    "    sum_H = torch.sum(tensor, dim=2)\n",
    "    sum_W = torch.sum(tensor, dim=1)\n",
    "    \n",
    "    # Find the indices with the largest sum\n",
    "    idx_H = torch.argmax(sum_H)\n",
    "    idx_W = torch.argmax(sum_W)\n",
    "    \n",
    "    # Return the corresponding slices\n",
    "    return idx_H, idx_W\n",
    "\n",
    "def compute_std_of_sum(tensor):\n",
    "    \"\"\"\n",
    "    Compute the standard deviation of the sum along H and W dimensions for the given image.\n",
    "    \n",
    "    Args:\n",
    "        tensor: A PyTorch tensor of shape (S, H, W).\n",
    "        \n",
    "    Returns:\n",
    "        A float of standard deviation of the sum along H and W dimensions for the given image.\n",
    "    \"\"\"\n",
    "    # Compute the sum along the H and W dimensions\n",
    "    sum_HW = torch.sum(tensor, dim=(1, 2))\n",
    "    \n",
    "    # Compute the standard deviation of the sum along the H and W dimensions\n",
    "    #print(tensor.shape, sum_HW.shape)\n",
    "    std_HW = torch.std(sum_HW, dim=0)\n",
    "    \n",
    "    return std_HW.item()\n",
    "\n",
    "def find_slices_within_std(tensor, t=1.0):\n",
    "    \"\"\"\n",
    "    Find the slices whose sum are within t standard deviations away from the max slice sum for the given image.\n",
    "    \n",
    "    Args:\n",
    "        tensor: A PyTorch tensor of shape (S, H, W).\n",
    "        t: A float representing the number of standard deviations away from the max slice sum to include.\n",
    "        \n",
    "    Returns:\n",
    "        A list containing the indices of the slices whose sum is within t standard deviations away from the max slice sum in the given image.\n",
    "    \"\"\"\n",
    "    # Compute the sum along the H and W dimensions\n",
    "    sum_HW = torch.sum(tensor, dim=(1, 2))\n",
    "    \n",
    "    # Compute the standard deviation of the sum for each image\n",
    "    std_sum = compute_std_of_sum(tensor)\n",
    "    \n",
    "    # Compute the max slice sum for the image\n",
    "    max_sum = torch.max(sum_HW, dim=0)[0]\n",
    "    \n",
    "    # Compute the threshold for including a slice\n",
    "    threshold = max_sum - t * std_sum\n",
    "    \n",
    "    # Find the slices whose sum is within the threshold for each image\n",
    "    indices_within_std = torch.where(sum_HW >= threshold)[0].tolist()\n",
    "    \n",
    "    if len(indices_within_std) < 3: # edge cases where too few slices are selected.\n",
    "        indices_within_std += [indices_within_std[-1] + 1, indices_within_std[0] - 1]\n",
    "    \n",
    "    return indices_within_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d623d45-d101-4c72-9f38-75b99eabfc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated using chat-gpt.\n",
    "import torch\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SlicesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset that selects v slices from 3D images within the range of standard deviations of the max slice sum.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_dataset, slices_within_std, v, transform=None):\n",
    "        \"\"\"\n",
    "        Initialize the SlicesDataset.\n",
    "        \n",
    "        Args:\n",
    "            base_dataset: A PyTorch Dataset that returns 3D images of shape (C, S, H, W).\n",
    "            slices_within_std: A list of N lists, where the i-th list contains the indices of the slices whose sum is within t standard deviations away from the max slice sum for the i-th image in the batch.\n",
    "            v: An integer representing the number of slices to select from each image.\n",
    "            transform: A function to be applied to both x and y when returning a value from __getitem__. Default is None.\n",
    "        \"\"\"\n",
    "        self.base_dataset = base_dataset\n",
    "        self.slices_within_std = slices_within_std\n",
    "        self.v = v\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Select v random slices from the 3D image within the range of standard deviations, and return a tensor of shape (C * v, H, W).\n",
    "        \n",
    "        Args:\n",
    "            index: An integer representing the index of the image to retrieve.\n",
    "            \n",
    "        Returns:\n",
    "            A tuple containing a PyTorch tensor of shape (C * v, H, W) and its corresponding label.\n",
    "        \"\"\"\n",
    "        # Get the original 3D image and its corresponding label\n",
    "        x_3d, y_3d, clin_data = self.base_dataset[index]\n",
    "        \n",
    "        # Get the indices of the slices within the range of standard deviations\n",
    "        indices_within_std = self.slices_within_std[index]\n",
    "        \n",
    "        # Randomly select v slices from the indices within the range of standard deviations\n",
    "        selected_indices = random.sample(indices_within_std, self.v)\n",
    "        \n",
    "        # Select the slices from the 3D image\n",
    "        x_slices = x_3d[:, selected_indices]\n",
    "        y_slices = y_3d[:, selected_indices]\n",
    "        \n",
    "        # Reshape the slices into C*v 2D tensors\n",
    "        x_2d = torch.reshape(x_slices, (-1, x_slices.shape[-2], x_slices.shape[-1]))\n",
    "        y_2d = torch.reshape(y_slices, (-1, y_slices.shape[-2], y_slices.shape[-1]))\n",
    "        \n",
    "        # Apply the transform function to both x and y\n",
    "        if self.transform:\n",
    "            x_2d, y_2d = self.transform(x_2d, y_2d)\n",
    "        \n",
    "        return x_2d, y_2d, clin_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the number of images in the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            An integer representing the number of images in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.base_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd10dc18-5371-4c4e-8604-11bbdddea0f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = train_ds_clin\n",
    "\n",
    "\n",
    "def get_slices_within_std_for_ds(ds, t=2.5):\n",
    "    # get slices with wmh sum within 3 std of the maximum\n",
    "    # I could also replace with for slices within 3 std of the max uncertainty?\n",
    "    slices_within_std = []\n",
    "    for data in tqdm(ds, position=0, leave=True):\n",
    "        y = data[1].squeeze()\n",
    "        slices_within_std.append(find_slices_within_std(y, t=t))\n",
    "        \n",
    "    return slices_within_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd4345af-22a9-4d02-bbb3-82b76e829230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#slices_within_std_for_ds = get_slices_within_std_for_ds(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc1182a8-2a07-4076-9b4d-7008c9125913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#(torch.Tensor([len(s) for s in slices_within_std_for_ds]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdaafd49-64bf-42b0-aa6f-ec6fbcbdcf74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### defining the augmentation procedure\n",
    "from trustworthai.utils.augmentation.standard_transforms import (\n",
    "    RandomFlip, GaussianBlur, GaussianNoise,\n",
    "    RandomResizeCrop, RandomAffine,\n",
    "    NormalizeImg, PairedCompose, LabelSelect,\n",
    "    PairedCentreCrop, CropZDim,\n",
    ")\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_transforms():\n",
    "    transforms = [\n",
    "        LabelSelect(label_id=1),\n",
    "        RandomFlip(p=0.5, orientation=\"horizontal\"),\n",
    "        # GaussianBlur(p=0.5, kernel_size=7, sigma=(.1, 1.5)),\n",
    "        # GaussianNoise(p=0.2, mean=0, sigma=0.2),\n",
    "        RandomAffine(p=0.2, shear=(-18,18)),\n",
    "        RandomAffine(p=0.2, degrees=15),\n",
    "        RandomAffine(p=0.2, translate=(-0.1,0.1)),\n",
    "        RandomAffine(p=0.2, scale=(0.9, 1.1)),\n",
    "#         #RandomResizeCrop(p=1., scale=(0.6, 1.), ratio=(3./4., 4./3.))\n",
    "\n",
    "#         #RandomResizeCrop(p=1., scale=(0.3, 0.5), ratio=(3./4., 4./3.)) # ssn\n",
    "    ]\n",
    "    transforms.append(lambda x, y: (x, y.squeeze().type(torch.long)))\n",
    "    return PairedCompose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ed2b8e5-9ecb-4f2d-b971-e3f2c08a6036",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### combine the clinical scores data into the x information.\n",
    "# generated with chatgpt\n",
    "class ClinicalDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_dataset, fields, target_field):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.fields = fields\n",
    "        self.target_field = target_field\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y, clin_data = self.base_dataset[index]\n",
    "        clin_data_fields = clin_data[self.fields].values\n",
    "        clin_data_tensor = torch.from_numpy(clin_data_fields.astype(np.float32))\n",
    "        target_field = clin_data[self.target_field]\n",
    "        return (x, clin_data_tensor), target_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6f6a949-0d1b-4d3c-962f-2ad87e3769f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch dataset that filters out nans\n",
    "class NonNanDataset(Dataset):\n",
    "    def __init__(self, original_dataset):\n",
    "        self.original_dataset = original_dataset\n",
    "        self.indices = []\n",
    "\n",
    "        for i in range(len(self.original_dataset)):\n",
    "            (x, clin_data), y = self.original_dataset[i]\n",
    "            if not (np.isnan(y) or torch.any(torch.isnan(clin_data))):\n",
    "                self.indices.append(i)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        original_index = self.indices[index]\n",
    "        return self.original_dataset[original_index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "561fa5e3-d938-4edb-840d-27666640b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RepeatDataset(Dataset):\n",
    "    def __init__(self, original_dataset, repeats):\n",
    "        self.original_dataset = original_dataset\n",
    "        self.repeats=repeats\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.original_dataset[idx % len(self.original_dataset)]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.original_dataset) * self.repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05a1a7f9-6410-4927-9b8e-308f800edcaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9448382c-3071-4566-8103-73098284e60f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### building the combined datasets with \n",
    "# augmentation transforms, added in umaps and predictions as channels\n",
    "# and code to select 3 slices from the image.\n",
    "\n",
    "def build_clinscores_prediction_dataset(ds, t=2.5, v=3, clin_fields=['age'], target_field='DWMH', apply_augmentation=False):\n",
    "    \n",
    "    # sort out the extra fields I need (e.g smoking one hot encoded)\n",
    "    ds = CustomClinFieldsDataset(ds)\n",
    "    \n",
    "    # load in the extra data (e.g umaps) to build a combined dataset\n",
    "    print(\"loading umaps and wmh preds images\")\n",
    "    output_maps_test = get_output_maps_for_ds(ds)\n",
    "    added_channels_ds = AddChannelsDataset(ds, output_maps_test)\n",
    "    \n",
    "    # compute which slices have wmh burdern with 3 std of the max\n",
    "    slices_within_std_for_ds = get_slices_within_std_for_ds(ds, t=t)\n",
    "    \n",
    "    # get dataset that randomly samples three slices for each input from within the range of slices within one std\n",
    "    new_ds = SlicesDataset(added_channels_ds, slices_within_std_for_ds, v=v, transform=get_transforms() if apply_augmentation else None)\n",
    "    \n",
    "    # combine the clin scores data into the x\n",
    "    new_ds = ClinicalDataset(new_ds, clin_fields, target_field)\n",
    "    \n",
    "    new_ds = NonNanDataset(new_ds)\n",
    "    \n",
    "    return new_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "20d1d258-c187-47e8-915f-0f3932dbe4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading umaps and wmh preds images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 42/42 [00:06<00:00,  6.19it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 42/42 [00:00<00:00, 158.51it/s]\n"
     ]
    }
   ],
   "source": [
    "t = 2.5\n",
    "v = 3\n",
    "# clin_fields = ['age', 'sex', 'diabetes']\n",
    "# clin_fields = ['age', 'sex', 'diabetes', 'hypertension', 'hyperlipidaemia', 'smoking_0', 'smoking_1', 'smoking_2', 'ICV']\n",
    "clin_fields = ['age', 'sex', 'diabetes', 'hypertension', 'hyperlipidaemia', 'smoking_0', 'smoking_1', 'smoking_2']\n",
    "target_field = \"DWMH\"\n",
    "new_test_ds = build_clinscores_prediction_dataset(test_ds_clin, t=t, v=v, clin_fields=clin_fields, target_field=target_field,  apply_augmentation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "8c31f4cc-aec9-416a-be3f-a49d294e8b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading umaps and wmh preds images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:04<00:00,  6.70it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 154.52it/s]\n"
     ]
    }
   ],
   "source": [
    "new_val_ds = build_clinscores_prediction_dataset(val_ds_clin, t=t, v=v, clin_fields=clin_fields)\n",
    "new_val_ds = RepeatDataset(new_val_ds, repeats=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7d109d95-e477-458c-9be1-e2a56f312809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading umaps and wmh preds images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [01:27<00:00,  2.06it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 179/179 [00:01<00:00, 172.41it/s]\n"
     ]
    }
   ],
   "source": [
    "new_train_ds = build_clinscores_prediction_dataset(train_ds_clin, t=t, v=v, clin_fields=clin_fields, apply_augmentation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "15a9f65e-27a3-46c4-a795-97b76dd2bfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### defining the dataloader\n",
    "\n",
    "batch_size = 12\n",
    "train_dataloader = DataLoader(new_train_ds, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(new_val_ds, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "test_dataloader = DataLoader(new_test_ds, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "811486e1-fc82-4c20-b832-d847ddf363d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x, clin_data), y = new_test_ds[0]#next(iter(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ae86c0c2-ef92-480b-812c-ad8e8bce764a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([21, 224, 160]), ())"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "04ef73d6-5200-48d6-b3ec-3ca2897eb50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "9ecf4d42-a0ae-492f-aae2-c8175e433d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clin_data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9fedf586-30c4-4102-9af3-8877831ae247",
   "metadata": {},
   "source": [
    "flair: 0-2\n",
    "mask: 3-5\n",
    "t1: 6-8\n",
    "var: 9-11\n",
    "ent: 12-14\n",
    "pred: 15-17\n",
    "seg: 18-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "78e94f2e-f234-4f0a-a2ee-9b398121737c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1f651869b0>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# this image can vary because of the slice sampling!\n",
    "plt.imshow(x[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ad0222a6-739f-4a88-b34b-a886014934e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1f804ffe-f381-4080-baf2-ca156c274ea1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import Any, Callable, List, Optional, Type, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=dilation,\n",
    "        groups=groups,\n",
    "        bias=False,\n",
    "        dilation=dilation,\n",
    "    )\n",
    "\n",
    "\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\" https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.0)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers: List[int],\n",
    "        num_classes: int = 1000,\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.init_inplanes = self.inplanes\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\n",
    "                \"replace_stride_with_dilation should be None \"\n",
    "                f\"or a 3-element tuple, got {replace_stride_with_dilation}\"\n",
    "            )\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck) and m.bn3.weight is not None:\n",
    "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
    "                elif isinstance(m, BasicBlock) and m.bn2.weight is not None:\n",
    "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
    "\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        planes: int,\n",
    "        blocks: int,\n",
    "        stride: int = 1,\n",
    "        dilate: bool = False,\n",
    "    ) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer\n",
    "            )\n",
    "        )\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(\n",
    "                block(\n",
    "                    self.inplanes,\n",
    "                    planes,\n",
    "                    groups=self.groups,\n",
    "                    base_width=self.base_width,\n",
    "                    dilation=self.dilation,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        #x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n",
    "def _resnet(\n",
    "    block: Type[Union[BasicBlock, Bottleneck]],\n",
    "    layers: List[int],\n",
    "    weights,#: Optional[WeightsEnum],\n",
    "    progress: bool,\n",
    "    **kwargs: Any,\n",
    ") -> ResNet:\n",
    "    if weights is not None:\n",
    "        _ovewrite_named_param(kwargs, \"num_classes\", len(weights.meta[\"categories\"]))\n",
    "\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "\n",
    "    if weights is not None:\n",
    "        model.load_state_dict(weights.get_state_dict(progress=progress))\n",
    "\n",
    "    return model\n",
    "\n",
    "def resnet18(*, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "    return _resnet(BasicBlock, [2, 2, 2, 2], None, progress, **kwargs)\n",
    "\n",
    "def resnet34(*, progress: bool = True, **kwargs: Any) -> ResNet:\n",
    "\n",
    "    return _resnet(BasicBlock, [3, 4, 6, 3], None, progress, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "622aeb85-bbe2-4b58-8e4f-39e2aa2a4167",
   "metadata": {},
   "outputs": [],
   "source": [
    "### selecting a model\n",
    "\n",
    "class PredictionModel(torch.nn.Module):\n",
    "    # uses a modified resnet that bypasses the usual \n",
    "    def __init__(self, image_channels, num_slices, num_clin_features, out_classes, latent_fc_features=64):\n",
    "        super().__init__()\n",
    "        model_base = resnet18()\n",
    "        model_base.conv1 = nn.Conv2d(image_channels*num_slices, model_base.init_inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        \n",
    "        self.model_base = model_base\n",
    "        \n",
    "        # replace the head of the model with another layer.\n",
    "        \n",
    "        self.fc1 = nn.Linear(model_base.fc.in_features + num_clin_features, latent_fc_features)\n",
    "        self.a = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(latent_fc_features, out_classes)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        \n",
    "        xid = {\"flair\":[0,1,2], \"mask\": [3,4,5], \"t1\":[6,7,8], \"var\": [9,10,11], \"ent\":[12,13,14], \"pred\":[15,16,17], \"seg\":[18,19,20]}\n",
    "        \n",
    "        x = inp[0]\n",
    "        \n",
    "        x = x[:,[*xid[\"flair\"], *xid[\"pred\"], *xid[\"ent\"]]]\n",
    "        # x = x[:,[*xid[\"flair\"], *xid[\"pred\"], *xid[\"var\"]]] \n",
    "        # x = x[:,[*xid[\"flair\"], *xid[\"pred\"], *xid[\"ent\"], *xid[\"var\"]]]\n",
    "        # x = x[:,[*xid[\"flair\"], *xid[\"pred\"]]]\n",
    "        # x = None\n",
    "        \n",
    "        # print(x.shape, [*xid[\"flair\"], *xid[\"pred\"]])\n",
    "        \n",
    "        clin_data = inp[1]\n",
    "        \n",
    "        if x != None:\n",
    "            features = self.model_base(x)\n",
    "        else:\n",
    "            features = torch.zeros(inp[0].shape[0], 512).cuda()\n",
    "        dense_input = torch.cat([features, clin_data], dim=1)\n",
    "        \n",
    "        #print\n",
    "        \n",
    "        out = self.fc2(self.a(self.fc1(dense_input)))\n",
    "        \n",
    "        return out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "58dc5169-b871-4ef7-a67e-2f23f0021a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will train a model that has access to:\n",
    "\n",
    "# clin scores\n",
    "# clin scores + flair + pred seg\n",
    "# clin scores + flair + pred seg + umap/vmap.\n",
    "# see how the accuracies of each perform. Nice. its just a prelim test, I don't need to do class re-weighting and all that stuff just yet.\n",
    "\n",
    "# I will try doing DWMH, PWMH, and then I can try adding in other classes, which can be done by just changing the ClinDataset class to have a differnt label\n",
    "# so could do Total and tab fazekas. If I can do tab fazekas well then I can predict total SVD better maybe?\n",
    "# see if the accuracy of the model does any better. I suspect that it might not though..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "1099c259-b5e1-4ffb-ac99-9320b8bd64b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 8])"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clin_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "bdeddd04-cc02-4b4c-b0e7-6b09f96fdd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_raw = PredictionModel(image_channels=3, num_slices=3, num_clin_features=8, out_classes=4)#.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "cdf5f346-1ab9-4bf3-8113-2397a3f8de34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class xent_wrapper(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base_loss = torch.nn.CrossEntropyLoss()\n",
    "    def forward(self, y_hat, y):\n",
    "        return self.base_loss(y_hat, y.type(torch.long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "909c447f-69ae-45f3-8c1a-135120a0d331",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = xent_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "b502ea83-cae6-4b20-84b4-7775c230c597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s2208943/miniconda3/envs/uq/lib/python3.10/site-packages/lightning_lite/plugins/environments/slurm.py:167: PossibleUserWarning: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python /home/s2208943/miniconda3/envs/uq/lib/python3.10/sit ...\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "### training the model\n",
    "# setup optimizer and model wrapper\n",
    "\n",
    "weight_decay = 0.0001#0.05\n",
    "max_epochs = 100\n",
    "lr=2e-4\n",
    "early_stop_patience = 15\n",
    "\n",
    "optimizer_params={\"lr\":lr, \"weight_decay\":weight_decay}\n",
    "optimizer = torch.optim.Adam\n",
    "lr_scheduler_params={\"milestones\":[1000], \"gamma\":0.5}\n",
    "lr_scheduler_constructor = torch.optim.lr_scheduler.MultiStepLR\n",
    "\n",
    "# wrap the model in the pytorch_lightning module that automates training\n",
    "model = StandardLitModelWrapper(model_raw, loss, \n",
    "                                logging_metric=lambda : None,\n",
    "                                optimizer_params=optimizer_params,\n",
    "                                lr_scheduler_params=lr_scheduler_params,\n",
    "                                optimizer_constructor=optimizer,\n",
    "                                lr_scheduler_constructor=lr_scheduler_constructor\n",
    "                               )\n",
    "\n",
    "# train the model\n",
    "trainer = get_trainer(max_epochs, \".\", early_stop_patience=early_stop_patience)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "ec3e7b12-d519-4203-9fd8-4ca112f9c409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s2208943/miniconda3/envs/uq/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:604: UserWarning: Checkpoint directory /home/s2208943/ipdis/UQ_WMH_methods/trustworthai/run/model_load exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type            | Params\n",
      "------------------------------------------\n",
      "0 | model | PredictionModel | 11.7 M\n",
      "1 | loss  | xent_wrapper    | 0     \n",
      "------------------------------------------\n",
      "11.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.7 M    Total params\n",
      "46.968    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s2208943/miniconda3/envs/uq/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1595: PossibleUserWarning: The number of training batches (15) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9ceb80c6c14c3eb84de181caeafdd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 0.786\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric val_loss did not improve in the last 15 records. Best score: 0.786. Signaling Trainer to stop.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "a20b974c-cf6a-4a0f-a861-6c1363752a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at /home/s2208943/ipdis/UQ_WMH_methods/trustworthai/run/model_load/epoch=0-step=15-v1.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from checkpoint at /home/s2208943/ipdis/UQ_WMH_methods/trustworthai/run/model_load/epoch=0-step=15-v1.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f1c15d2aa194235a589036bd9c8b70f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "     Validate metric           DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        val_loss            0.7918273210525513\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.7918273210525513}]"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(model, val_dataloader, ckpt_path='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "c910d4de-9fc9-45e1-aeb5-608b8c74d9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.listdir(\"/home/s2208943/ipdis/UQ_WMH_methods/trustworthai/run/model_load/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "c7dafe6e-e7cb-446a-bd21-c2dba782db2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(9, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.model_base.conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "24b35c5c-c0bd-443d-b32f-8e3e4294347a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so: my three metrics that I want to compute are: top k, mode accuracy form 15 runs, accuracy over 15 runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "a84acef4-a363-43a3-b6dc-ceab56e25f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_metric = torchmetrics.F1Score(task='multiclass', num_classes=4) # note need to set the F1 score correctly depending on the number of classes in the task!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "b51afefe-90b1-4be0-8a66-8d6104bbced6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:45<00:00,  1.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 42])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# since the val dataloader does not have shuffle, its actually easy to take the\n",
    "# mode...\n",
    "all_y_hats = []\n",
    "accs = []\n",
    "top2_accs = []\n",
    "\n",
    "batch_accs = []\n",
    "\n",
    "runs = 30\n",
    "model.eval()\n",
    "\n",
    "for _ in tqdm(range(runs), position=0, leave=True):\n",
    "    y_hat_preds = []\n",
    "    \n",
    "    for i, ((x, clin_data), y) in enumerate(test_dataloader):\n",
    "        with torch.no_grad():\n",
    "            pred = torch.nn.functional.softmax(model.cuda()((x.cuda(), clin_data.cuda())), dim=1).cpu()\n",
    "            y_hat = pred.argmax(dim=1)\n",
    "            \n",
    "            y_hat_preds += list(y_hat)\n",
    "            \n",
    "            acc = (y == y_hat).sum() / len(y)\n",
    "            accs.append(acc)\n",
    "            \n",
    "            top2 = pred.topk(dim=1, k=2)[1]\n",
    "            top2_acc = ((y == top2[:,0]) | (y == top2[:,1])).sum() / len(y)\n",
    "            top2_accs.append(top2_acc)\n",
    "            \n",
    "    all_y_hats.append(y_hat_preds)\n",
    "\n",
    "# calculate the mode accuracy (which is usually higher than mean over multiple runs but can vary quite a bit...\n",
    "print(torch.Tensor(all_y_hats).shape)\n",
    "mode_preds = torch.Tensor(all_y_hats).mode(dim=0)[0]\n",
    "all_ys = []\n",
    "for i, ((x, clin_data), y) in enumerate(test_dataloader):\n",
    "    all_ys += list(y)\n",
    "all_ys = torch.Tensor(all_ys)\n",
    "\n",
    "mode_acc = (mode_preds == all_ys).sum() / len(all_ys)\n",
    "runs_mean_acc = torch.Tensor(accs).mean()\n",
    "runs_mean_top2 = torch.Tensor(top2_accs).mean()\n",
    "\n",
    "# f1 on mode predictions\n",
    "mode_f1 = f1_metric(mode_preds, all_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "b092007b-87b8-4520-8ed5-4eebb5013ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode acc: 0.7857142686843872, run mean acc: 0.7479166388511658, top-2 acc:0.9527778029441833\n"
     ]
    }
   ],
   "source": [
    "# this is with + ent\n",
    "print(f\"mode acc: {mode_acc}, run mean acc: {runs_mean_acc}, top-2 acc:{runs_mean_top2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "8bbae5c2-9aea-4243-bef9-17dce549f107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode acc: 0.738095223903656, run mean acc: 0.7486110925674438, top-2 acc:0.9284722208976746, mode f1: 0.738095223903656\n"
     ]
    }
   ],
   "source": [
    "# this is with just the seg, another run on same model....\n",
    "print(f\"mode acc: {mode_acc}, run mean acc: {runs_mean_acc}, top-2 acc:{runs_mean_top2}, mode f1: {mode_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "51ad4809-5d37-4ec3-8aa6-b7fdf724e7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode acc: 0.7857142686843872, run mean acc: 0.7736110687255859, top-2 acc:0.9576388001441956\n"
     ]
    }
   ],
   "source": [
    "# this is with + ent, another run on the same model....\n",
    "print(f\"mode acc: {mode_acc}, run mean acc: {runs_mean_acc}, top-2 acc:{runs_mean_top2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "ad080501-1c84-4a36-b089-7233198f67c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode acc: 0.761904776096344, run mean acc: 0.7166667580604553, top-2 acc:0.913194477558136\n"
     ]
    }
   ],
   "source": [
    "# this is with + var map\n",
    "print(f\"mode acc: {mode_acc}, run mean acc: {runs_mean_acc}, top-2 acc:{runs_mean_top2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3435a11c-399b-4260-b548-961e344fad2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode acc: 0.7142857313156128, run mean acc: 0.736111044883728, top-2 acc:0.9395833611488342, mode f1: 0.7142857313156128\n"
     ]
    }
   ],
   "source": [
    "# this is with just the seg, another run on same model....\n",
    "print(f\"mode acc: {mode_acc}, run mean acc: {runs_mean_acc}, top-2 acc:{runs_mean_top2}, mode f1: {mode_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "id": "cb1a280e-eddd-48f4-8490-6574a38bab7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode acc: 0.7142857313156128, run mean acc: 0.7364583015441895, top-2 acc:0.9125000238418579\n"
     ]
    }
   ],
   "source": [
    "# this is with just the seg\n",
    "print(f\"mode acc: {mode_acc}, run mean acc: {runs_mean_acc}, top-2 acc:{runs_mean_top2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "e35dd6e0-80d7-4fbd-b954-d4fe8fde783a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 1, 3, 2, 1, 0, 1, 2, 3, 0, 3])"
      ]
     },
     "execution_count": 569,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(12, 4)\n",
    "a.topk(dim=1, k=2)[1][:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187c9e13-11f7-415a-8361-ef906ff9e733",
   "metadata": {},
   "outputs": [],
   "source": [
    "### changes\n",
    "\n",
    "# validation and test have no augmentation\n",
    "# validation is made to repeat over itself 4 times for a less nooisy val choice.\n",
    "# regularization is increased to 0.05, set a max number of epochs to 30, pick best on val\n",
    "# pick the mode prediction over 50 runs instead of 30.\n",
    "\n",
    "# no but sometimes the model fails so we need the val dataloader. so maybe\n",
    "# I should make the repeats of the val dataloader much bigger, like * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "17d7e778-7d52-4bbf-ab8f-695f330f6d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accs = []\n",
    "\n",
    "# runs = 15\n",
    "# for _ in range(runs):\n",
    "#     for (x, clin_data), y in test_dataloader:\n",
    "#         with torch.no_grad():\n",
    "#             pred = torch.nn.functional.softmax(model.cuda()((x.cuda(), clin_data.cuda())), dim=1).cpu()\n",
    "#             acc = (y == pred.argmax(dim=1)).sum() / len(y)\n",
    "#             print(acc)\n",
    "#             accs.append(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "595dc146-2288-4d20-9df8-a9255accb8da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7069)"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is with + ent (another run, lets see the variance)\n",
    "torch.stack(accs).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "bb8d26a8-980f-4bec-91bb-834cedd319eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7292)"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is with + ent\n",
    "torch.stack(accs).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "591030f5-9f5b-4547-8a25-cebc41b234ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6792)"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is with just the seg\n",
    "torch.stack(accs).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "86f4c639-642f-41f8-b214-47f1665921cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6042)"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is clin data only\n",
    "torch.stack(accs).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "0b128a0c-7ce5-4f91-b9cb-92120d77fe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# two things. \n",
    "# [x] 1) I need to put in all the clinical scores (including volume and damage scores maybe?)\n",
    "# [ ] 2) I need to cross validate because I'm not sure if the overall results are just quite good when cross validating (and I should compare to 6 fold cross validation of the logistic regression model for a fair comparison)\n",
    "# [x] 3) I need to compute over the same data a bunch of times and take the mode (and try taking the mode top-k as well for some actually good analysis).\n",
    "# [ ] 4) I can also try with just a single linear regression layer, that way it matches more with the lgositic regression model I am using.\n",
    "# [ ] 5) I also need to try predicting other things, such as PVWMH, total fazekas, tab fazekas, and get a whole training pipeline going. Nice.\n",
    "# [ ] 6) at the earliest opportunity, see if I have statistical significance? Then I know if I am not wasting my time (and compare to just the logistic regression model... \n",
    "#    its an interesting result if just a logistic regression models with just mean seg does as well as the nn, like the whole map doesn't really add anything...\n",
    "# [ ] 7) compare to using the binarized segmentation instead of the pred for the mean model....\n",
    "# [ ] 8) I was also using age normalized and a total prediction of WMH mass in the logistic regression model..., so those features could actually be useful.... including WMH mass, umap mass etc.\n",
    "# [ ] 9) the validation dataset effectively changes each iteration and it is small, so the early stopping is very noisy. perhaps I should just take the model after 20 epochs instead and see\n",
    "# what the difference is, because if it lucks out on epoch two then I'm not gonna get a very good model.\n",
    "# [x] 10) training with just the clinscores, no extra information.\n",
    "# [ ] 11) training with a fixed number of epochs (because there is a high level of stochasticity in the early stopping validation score... would also give a bigger test set.... worth a try).\n",
    "# [ ] 12) using a pretrained model and 'finetuning'."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d752d56-1689-410d-9292-5e4aa6554bc4",
   "metadata": {},
   "source": [
    "1) - adding in the other clinical scores. Okay, in the logistic regression model I used: \n",
    "\n",
    "'age', 'sex', 'diabetes', 'hypertension', 'hyperlipidaemia', 'ICV', I also need to add smoking, its an important covariate I think. so I need to one-hot-encode smoking...\n",
    "So, my model with just the mean seg got better, but my model with the uncertianty map wasn't a great run. so things to go forward:\n",
    "\n",
    "- take out ICV? take out smoking? normalize age?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "03d8ccad-51d8-4d6f-bb4e-a63f27f6cce1",
   "metadata": {},
   "source": [
    "6) + 2) significance testing. I will need to compute the values for each fold and do a significance test on my k fold cross validation. However, I could also compute the\n",
    "values of interest per batch, then I would have k * batches roughly of values to compare for my significance test. Nice. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5a91b34-ece3-4ee0-b8a8-884a1f2c9da0",
   "metadata": {},
   "source": [
    "3) - taking the mode of the predicitons, do I actually get a performance difference then? Yes, its always higher (and the difference between using uncertainty and not\n",
    "is actually bigger, which is quite interesting....)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1199cdce-8ff6-4e76-945d-37e097e68d6b",
   "metadata": {},
   "source": [
    "10) - mode, trained on just clin scores\n",
    "done it - accuracy is suprisingly high for the 2 layer nn model than the logistic regression model. (but also it does just predict class 1 each time for the split I was looking at....)\n",
    "it was getting around 0.6 accuracy, so im guessing 0.6 of the inputs are class 1....\n",
    "to discuss this issue I can talk about the difference between uncertianty levels for deep fazekas 3 vs 1 and 0, and show clear separation, but when we introduce 2 it becomes much more blurry."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d26a8260-3d72-412a-be13-36cae2cc3cf7",
   "metadata": {},
   "source": [
    "I need to look at the mode prediction, but for now lets look at the accuracy over 15 runs I think."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c5644dea-8de2-4999-a2cc-fe0cc810173f",
   "metadata": {},
   "source": [
    "## issue about the quality of predictions\n",
    "\n",
    "its getting slightly less than the linear regression models were. But that is partly because these models have access to less data (I have a val and a test split here), and are more prone to overfitting\n",
    "due to the larger number of parameters etc. The results are pretty conclusive actually, and that will be a statistically significant result. Nice!!!\n",
    "\n",
    "I need to point out that these models actually perform better than the linear regression models despite the imperfect (and arguably a bit arbitrary?) slice choice mechanism.\n",
    "\n",
    "# because my splits are not even sizes, then I can calculate the evaluation results for each individual separately for each fold, and then \n",
    "# recalculate all my metrics on new splits of the data (I can just divide the data into 10 even splits for example and compute the p-values that way. Nice). Okay, this is good\n",
    "now, I should train 3 models and pick the best of the three models that I train for each fold for each task. nice, shouldn"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f83964c5-eec4-49d8-83ce-d013dad4e6e6",
   "metadata": {},
   "source": [
    "## What I will want in my final analysis\n",
    "show top 2, mode and f1 score on the mode, with significance testing. So I am going to need to keep track of\n",
    "the mode predictions for each model, the actual y for each model, and do this per batch as well as per fold then I have more values to compare?\n",
    "I should also do this three times and take the results of the best model?\n",
    "and I should increase the number of samples I do to say 30 to reduce the variance?\n",
    "Maria likes to see ROI curve for each plot.\n",
    "ROI curve and confusion matrix.\n",
    "I can also put the in the plot of as we go above the ventricles, how much uncertainty mass do we see, and show clear separation of fazekas 0, 1, and 3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
